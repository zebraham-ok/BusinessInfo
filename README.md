# BusinessInfo程序开发文档

## 1. 概述
本爬虫系统旨在高效地从网络上采集数据，并通过多并发处理实现高吞吐量。目标是每天能够处理超过5万条数据。

![开发说明](img\开发说明.png)

## 2. 系统架构

### 2.1 检索获取URL
获取途径有三种：可检索的网页列表、搜索引擎API、监控网页列表。其中，可检索的网页列表、搜索引擎API这两个是需要检索词的，监控网页列表不需要检索词。

#### 2.1.1 检索词生成
- **SQL**: 从SQL中获取一个尚未检索的公司名称。
- **检索词生成**: 根据获取的公司名称生成检索词，可以用大语言模型生成多种检索词。

#### 2.1.2 可检索网页列表
+ 可检索网页列表是财经媒体、百科词条等拥有海量信息的平台，对他们适合采取主动检索而非监控的策略
+ 尚未构建，参考资料 `web_pag_doc\model_url.py`

#### 2.1.3 搜索引擎API
+ 使用搜索引擎API，可以广泛获得各种渠道的信息，扩充优质网页的来源。
+ 有两种选择，Google检索可以指定检索的时间范围，而Bing不能。Google声称每个Key每天只能用100次。

#### 2.1.4 监控网页列表
+ 监控网页列表是一些列提供供应链信息的高质量来源，他们定期更新，适合进行长期监控。
+ 尚未构建，参考资料 `web_pag_doc\url_count.xlsx`


#### 2.1.4 Jina
- **可检索网页列表**: 使用[Jina Reader](https://jina.ai/)查询监控网页列表，
- **3s以上间隔免费**

### 2.2 AI规划器
+ 最粗糙的方式是所有链接都爬
+ 之后首先实现对于url的类型进行一个识别，只爬取必要的
+ 最终可能会使用Agent技术

### 2.3 任务队列
+ 使用RabbitMQ，将规划器决定要爬取的url存在队列中
+ 每个爬取器线程从这个队列当中拿出一个来爬

### 2.4 爬取器
+ 需要有并发能力目前使用的是threading
+ 现有的代码的思路：
    + 先request，后Selenium
    + 爬取到之后用Beautiful Soup处理，直接摘取HTML中最长的文本（但是这可能存在问题，比如最长的文本其实只是全文当中的一个段落）
+ 最理想的做法是对典型的网站建立模板，让爬取器知道应该到哪里找正文（目前仅对msn新闻做了一个专门的msn_spider）

### 2.5 图表文本化
+ 爬取网页的时候，图片应该是以链接的形式存储的（表格是什么形式？可能在JavaScript里面）
+ 使用多模态大模型，将图片中

### 2.6 文档初筛
使用一些人工智能方法对已经存入SQL的纯文本进行一些分类、筛选
+ 主题分类
+ 质量分类
+ Simhash索引

## 3 环境说明
+ 系统最好兼容Windows和Linux，初版会在Windows上线运行，以后可能会考虑迁移到Linux
+ 设计能力是每天爬取5万条以上

## 4 注意事项
+ 代码要有充分的注释
+ 长文本尽量使用：
    '''
        text
    '''
    多行存储
+ 每个函数要用一行字符串进行说明，让鼠标放在这个函数上有显示，对每个变量也要进行解释
+ 复杂功能尽量使用类封装
+ 对于多线程任务，注意资源泄漏问题
